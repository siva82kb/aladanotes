% !TEX root = ../notes_template.tex
\chapter{Matrices}\label{chp:vectors}

% \minitoc

\section{Matrcies}
Matrices are rectangular arrangement of numbers, with a finite number of rows and columns. Matrices are represented as rectangular grid of numbers with $n$ rows and $m$ columns, with the grid contained between two square brackets, as shown below:
%\vspace{0.15cm}
\begin{center}
\begin{tikzpicture}[scale=0.6]
\draw[thick, ->] (0,0) -- (0, -4.3) node[midway,left] {$n$ rows};
\draw[thick, ->] (0.5,0.5) -- (6.1, 0.5) node[midway,above]{$m$ columns};
%\node[xshift=-0.6cm,yshift=-1.3cm] {Rows};
\node[gray,xshift=2.0cm,yshift=-1.3cm] {$\begin{bmatrix}
\Box & \Box & \Box & \ldots & \Box \\
\Box & \Box & \Box & \ldots & \Box \\
\Box & \textcolor{black}{a_{32}} & \Box & \ldots & \Box \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\Box & \Box & \Box & \ldots & \Box \\
\end{bmatrix}$};
\draw[draw=red] (0.5,-2.35) rectangle ++(5.6,0.8);
\draw[draw=blue] (1.6,-4.25) rectangle ++(1.0,4.25);
\draw [thick, blue, ->] (2.15,-4.4) to [out=-90,in=180] (3.0, -5.0) node[right] {\small{$2^{nd}$ column}};
\draw [thick, red, ->] (6.2,-1.95) to (7.,-1.95) node[right] {\small{$3^{rd}$ row}};
\end{tikzpicture}\hspace{0.5cm}
\end{center}
The matrix is referred to as an $n \times m$ matrix, where $n$ is the number of rows and $m$ is the number of columns. The numbers in the matrix are called the \textit{elements} of the matrix. The element in the $i^{th}$ row and $j^{th}$ column of the matrix is denoted by $a_{ij}$. The elements of a matrix with the same row and column index are called the \textit{diagonal elements} of the matrix; these are elements of the form $a_{ii}$.

We will denote matrices using bold capital letters, and its elements by the corresponding lowercase letter with the row and column indices as subscripts; the row index will be written first, followed by the column index. For example, in the picture above $a_{32}$ is the element corresponding to the element in the $3^{rd}$ row and $2^{nd}$ column. This must make you wonder if the $n$-vectors we talked about earlier are just $n \times 1$ matrices. You are right! We can interpret these as $n \times 1$ matrices. In fact, these single column matrices are also referred to as \textit{column vectors} or a \textit{column matrix}. Now hold on, does that mean that we can also have \textit{row vectors} or \textit{row matrices}? Yes, we do have row vectors, which are $1 \times m$ matrices. We will talk about these in a later section in this chapter. Note that we can also have a $1 \times 1$ matrix!

Depending on the number of rows and columns, we group matrices in three categories based on their shape:
\begin{itemize}
    \item \textbf{Square matrix:} A matrix is said to be square if it has the same number of rows and columns, $n = m$. A square matrix is denoted by $n \times n$.
    \item \textbf{Wide/Fat matrix:} A matrix with more columns than rows, $n < m$.
    \item \textbf{Tall/Skinny matrix:} A matrix with more rows than columns, $n > m$.
\end{itemize}
We will refer to $n \times m$ as the \textit{shape of a matrix} $\mf{A}$, where $n, m$ are the number of rows and columns of $\mf{A}$, respectively. The set of all $n \times m$ matrices is denoted by the set $\mb{R}^{n \times m}$, where $\mb{R}$ is the set of real numbers. Later on we will come across matrices where the elements are complex numbers and these would be elements from the set $\mb{C}^{n \times m}$.

\noindent\textbf{Block Matrices and Submatrices}: We will often also come across matrices where the elements themselves are matrices. These are called \textit{block matrices}. The following is an example of a block matrix $\mf{M}$:
\begin{equation}
    \mf{M} = \bmxc \mf{A}_1 & \mf{A}_2 \\ \mf{A}_3 & \mf{A}_4 \emx
    \label{eq:ch02-block-mat}
\end{equation}
Here, $\mf{A}_1, \mf{A}_2, \mf{A}_3, \mf{A}_4$ are themselves matrices, which are refered to as the submatrices of $\mf{M}$. We cannot have any arbitrary matrices as submatrices of a block matrix. The submatrices must satisfy some constraints.
\begin{itemize}
    \item The submatrices in a column must have the same number of columns, but have arbitrary number of rows.
    \item The submatrices in a row must have the same number of rows, but have arbitrary number of columns.
\end{itemize}
Let the shape of the submatrix $\mf{A}_i$ in Eq.~\ref{eq:ch02-block-mat} be $n_i \times m_i$. Then, $n_1 = n_2$, $n_3 = n_4$, $m_1 = m_3$, and $m_2 = m_4$. The shape of the block matrix $\mf{M}$ is $\pp{n_1 + n_3} \times \pp{m_1 + m_2}$.

Matrices also are a conveninent way of represent a set of indexed column $n$-vectors, $\mf{x}_1, \mf{x}_2, \ldots \mf{x}_m$. We can treat these columns vectors are $n \times 1$ matrices and form a block matrix as shown below:
\begin{equation}
    \mf{X} = \bmxc \mf{x}_1 & \mf{x}_2 & \ldots & \mf{x}_m \emx
    \label{eq:ch02-block-col-vec}
\end{equation}
\textcolor{red}{Can I call this matrix a block row matrix? What is the shape of this matrix?}
\subsection{Some special matrices}
We will now define some special matrices that we will come across in this course.
\begin{itemize}
    \item \textbf{Zero matrix:} The matrix whose elements are all zeros is called the \textit{zero matrix}. These are often represented by $\mf{0}_{n \times m}$ --  the matrix of shape $n \times m$ with all elements as zeros.
    \item \textbf{Identity matrix:} The square matrix whose diagonal elements are all ones and all other elements are zeros is called the \textit{identity matrix}. The identity matrix of shape $n \times n$ is denoted by $\mf{I}_n$.
    \[ \mf{I}_2 = \bmxc 1 & 0 \\ 0 & 1 \emx \quad \mf{I}_3 = \bmxc 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \emx \]
    Notice that we can also represent identify matrices as a block row matrix of the following form,
    \[ \mf{I}_n = \bmxc \mf{e}_1 & \mf{e}_2 & \ldots & \mf{e}_n \emx \]
    where, $\mf{e}_1, \mf{e}_2, \ldots \mf{e}_n$ are the $n$-unit vectors of $\mb{R}^n$.
    \item \textbf{Diagonal matrix:} A square matrix whose non-diagonal elements are all zeros is called a \textit{diagonal matrix}. The diagonal matrix of shape $n \times n$ with diagonal elements $d_1, d_2, \ldots d_n$ is denoted by $\mf{D}$.
    \[ \bmxc d_1 & 0 & \ldots & 0 \\ 0 & d_2 & \ldots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \ldots & d_n \emx = \textbf{diag}\pp{d_1, d_2, \dots d_n}\]
    \item \textbf{Upper triangular matrix:} A square matrix whose elements below the diagonal are all zeros is called an \textit{upper triangular matrix}.
    \item \textbf{Lower triangular matrix:} A square matrix whose elements above the diagonal are all zeros is called a \textit{lower triangular matrix}.
    \item \textbf{Sparse matrix:} A matrix with a large number of zeros is called a \textit{sparse matrix}. We will not come across sparse matrices in this course.
\end{itemize}

\subsection{Why do I need to know about matrices?}
Matrices are a fundamental concept in linear algebra, and are used in many applications. There are two ways to interpret a matrix -- the rectangular arrangment of numbers:

\vspace{0.2cm}
\noindent \textbf{Data representation:} Matrices are a convenient way to represent data. Any application where there is a set of parameters are measured multiple times - across space, time, individuals, etc. These are usually represented as a table of entires. A table of entries can be thought of as a matrix. For example, 
\begin{itemize}
    \item The temperature recorded at different locations at different times can be represented as a matrix; the different locations could correspond to the columns and different measurement timepoints could correspond rows.
    \item The clinical information of patients visiting a hospital can be represented as a matrix. The different patient details could the columns and the rows could correspond to different patients.
    \item A grayscale image is a matrix, with each element representing the the intensity of a pixel located at a partiular horizontal and vertical position.
    \item and so on $\ldots$
\end{itemize}

\vspace{0.2cm}
\noindent \textbf{Linear transformations:} Matrices are used to represent linear transformations. A linear transformation is a function that maps a vector to another vector, which means these can be use to manipulate vectors. We will have a detailed discussion on this section \hl{xxx}.

\section{Matrix operations}
We will now define some important matrix operations involving matrices. These are the operations that we will use in this course.

\subsection{Matrix transpose}
This may sound like a strange operation at first, but it turns out be an important operation. The transpose of a matrix is obtained by interchanging the rows and columns of the matrix. The transpose of a matrix $\mf{A} \in \mb{R}^{n \times m}$ is denoted by $\mf{A}^\top$. This matrix is a member of the set $\mb{R}^{m \times n}$. The element in the $i^{th}$ row and $j^{th}$ column of the matrix $\mf{A}$ is the $j^{th}$ row and $i^{th}$ column of the matrix $\mf{A}^\top$. The transpose of a matrix is defined as:
\[ \mf{A} = \bmxc a_{11} & a_{12} & \ldots & a_{1m} \\ a_{21} & a_{22} & \ldots & a_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \ldots & a_{nm} \emx \longrightarrow \mf{A}^\top = \bmxc a_{11} & a_{21} & \ldots & a_{n1} \\ a_{12} & a_{22} & \ldots & a_{n2} \\ \vdots & \vdots & \ddots & \vdots \\ a_{1m} & a_{2m} & \ldots & a_{nm} \emx \]

\begin{boxedstuff}
    \begin{problem}
        What is the transpose of the block matrix $\mf{M} = \bmxc \mf{A}_1 & \mf{A}_2 \\ \mf{A}_3 & \mf{A}_4 \emx$?.
    \end{problem}
\end{boxedstuff}

\subsection{Matrix scalar multiplication}
We can also multiply a matrix by a scalar. Given a scalar $c \in \mb{R}$ and a matrix $\mf{A} \in \mb{R}^{n \times m}$, the scalar multiplication operation produces another matrix $c\mf{A}$ whose elements are $ca_{11}, ca_{12}, \ldots, ca_{nm}$. The scalar multiplication operation is defined as:
\begin{equation}
    c\mf{A} = c\bmxc a_{11} & a_{12} & \ldots & a_{1m} \\ a_{21} & a_{22} & \ldots & a_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \ldots & a_{nm} \emx = \bmxc ca_{11} & ca_{12} & \ldots & ca_{1m} \\ ca_{21} & ca_{22} & \ldots & ca_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ ca_{n1} & ca_{n2} & \ldots & ca_{nm} \emx \in \mb{R}^{n \times m}
    \label{eq:ch02-mat-scalar-mult}
\end{equation}

\subsection{Matrix addition}
We can define a matrix addition operation between two matrices. We can add two matrices only if they have the same shape, i.e. they have the same number of rows and same number of columns. Consider two matrices $\mf{A}, \mf{B} \in \mb{R}^{n \times m}$. The addition of these matrices is defined as follows:
\begin{equation}
    \begin{split}
    \mf{A} + \mf{B} &= \bmxc a_{11} & a_{12} & \ldots & a_{1m} \\ a_{21} & a_{22} & \ldots & a_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \ldots & a_{nm} \emx + \bmxc b_{11} & b_{12} & \ldots & b_{1m} \\ b_{21} & b_{22} & \ldots & b_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ b_{n1} & b_{n2} & \ldots & b_{nm} \emx \\    
    &= \bmxc a_{11} + b_{11} & a_{12} + b_{12} & \ldots & a_{1m} + b_{1m} \\ a_{21} + b_{21} & a_{22} + b_{22} & \ldots & a_{2m} + b_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} + b_{n1} & a_{n2} + b_{n2} & \ldots & a_{nm} + b_{nm} \emx \in \mb{R}^{n \times m}
    \end{split}
    \label{eq:ch02-mat-add}
\end{equation}    
Its very simple. You simply add the individual elements of the two matrices to get the elements of the resulting matrix. The resulting matrix is also a member of the set $\mb{R}^{n \times m}$. Note that this is consistent with the definition of the vector addition operation defined in the previous chapter.

\subsubsection{Properties of matrix addition}
Here are some of the properties of matrix addition:
\begin{itemize}
    \item \textbf{Commutative property:} $\mf{A} + \mf{B} = \mf{B} + \mf{A}$.
    \item \textbf{Associative property:} $\mf{A} + \pp{\mf{B} + \mf{C}} = \pp{\mf{A} + \mf{B}} + \mf{C}$.
    \item \textbf{Distributive property:} $c\pp{\mf{A} + \mf{B}} = c\mf{A} + c\mf{B}$.
    \item \textbf{Additiob with the zero matrix:} $\mf{A} + \mf{0}_{n \times m} = \mf{A}$.
    \item \textbf{Tranpose of the sum:} $\pp{\mf{A} + \mf{B}}^\top = \mf{A}^\top + \mf{B}^\top$.
\end{itemize}    
We leave it as a exercise for you to prove these properties using the properties of real number addition.

\begin{boxedstuff}
    \begin{problem}
        \textbf{The set of matrices $\mf{R}^{n\times m}$ for a vector space!} The set of $n \times m$ matrices with real numbers is the set $\mb{R}^{n \times m}$. We have defined scalar matrix multiplication and matrix addition operations on the elements of this set. Show that the set $\mb{R}^{n \times m}$ is a vector space. 
        
        \noindent\begin{small}\textcolor{gray}{\textit{Hint}: You just need to show the set of closed under scalar multiplication and vector addition. You can also verify that the additional properties are also satisfied by this set.}\end{small}
    \end{problem}
\end{boxedstuff}

\subsection{Matrix multiplication}
This is the most important operation involving matrices. Understanding matrix multiplication is vital to understanding the rest of the course material. So the importance of this section cannot be overstated. Unlike matrix scalar multiplication and matrix addition, the concept of matrix multiplication will seem a bit strange at first. But we will see later that the definition of matrix multiplication is a natural when matrices are viewed as reprensenting linear transformations. 

Consder two matrices $\mf{A} \in \mb{R}^{n \times m}$ and $\mf{B} \in \mb{R}^{p \times q}$. A matric multiplication operation $\mf{A}\mf{B}$ is defined if and only if the number of columns of $\mf{A}$ is equal to the number of rows of $\mf{B}$, i.e. $m = p$. The result of the matrix multiplication operation is a matrix $\mf{C} \in \mb{R}^{n \times q}$, where $\mf{C} = \mf{A}\mf{B}$. The elements of the resulting matrix $\mf{C}$ are defined as:
\begin{equation}
    c_{ij} = \sum_{k=1}^{m} a_{ik}b_{kj}, \,\, \forall i \in \lc 1, \ldots n\rc, \,\, j \in \lc 1, \ldots q\rc
    \label{eq:ch02-mat-mult}
\end{equation}
This for sure looks confusing and makes little sense. It turns out the matrix multiplication operation is a natural operation representing composition of linear transformation, and all matrices represent linear transformations. 

\begin{boxedstuff}
    \vspace{4mm}
    \noindent{\large \textbf{Hadamard product: Element-wise matrix multiplication} }
    \hrule
    \vspace{2mm}

    On a separate note, you are probably asking yourself, why not just define matrix multiplication like we defined matrix addition. Just multiply the individual elements together. the element-wise multilication is also a useful operation and is supported by scientific computing programs like MATLAB, Python, etc. This operation is called the \textit{Hadamard product} and is denoted by $\circ$. The Hadamard product of two matrices $\mf{A}, \mf{B} \in \mb{R}^{n \times m}$ is defined as:
    \begin{equation}
        \mf{A} \circ \mf{B} = \bmxc a_{11} & a_{12} & \ldots & a_{1m} \\ a_{21} & a_{22} & \ldots & a_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \ldots & a_{nm} \emx \circ \bmxc b_{11} & b_{12} & \ldots & b_{1m} \\ b_{21} & b_{22} & \ldots & b_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ b_{n1} & b_{n2} & \ldots & b_{nm} \emx = \bmxc a_{11}b_{11} & a_{12}b_{12} & \ldots & a_{1m}b_{1m} \\ a_{21}b_{21} & a_{22}b_{22} & \ldots & a_{2m}b_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1}b_{n1} & a_{n2}b_{n2} & \ldots & a_{nm}b_{nm} \emx \in \mb{R}^{n \times m}
        \label{eq:ch02-mat-hadamard}
    \end{equation}
    This element-wide multiplication operation is useful when matrices are used to represent data. For example, we might use the Hadamard product when wish to apply a mask to a image represented by the matrix. 
    
    We will not discuss the Hadamard product in this course, but it is good to know that this operation exists.
    We will not discuss the Hadamard product in this course, but it is good to know that this operation exists.
\end{boxedstuff}

The definition of matrix multiplication in Eq.~\ref{eq:ch02-mat-mult-ip} is hardly illuminating. Its hard to see what is going on. Before we dive a littel deeper into matrix multiplication, we will first define some coventions we will follow in the course:
\begin{itemize}
    \item All $n$-vectors are will be column vectors. Row vectors will be represented as the transpose of a column vector.
    \item For a matrix $\mf{A} \in \mb{R}^{n \times m}$, the columns of the matrix will be represented as $\mf{a}_1, \mf{a}_2, \ldots \mf{a}_m$, where each vector is from $\mb{R}^n$. Then, we can write the matrix as the following,
    \begin{equation}
        \mf{A} = \bmxc \mf{a}_1 & \mf{a}_2 & \ldots & \mf{a}_m \emx
        \label{eq:ch02-mat-col-rep}
    \end{equation}
    We represent the rows of the matrix $\mf{A}$ as $\tilde{\mf{a}}^\top_1, \tilde{\mf{a}}^\top_2, \ldots \tilde{\mf{a}}^\top_n$, where each vector $\tilde{\mf{a}}_i$ is a column vector from $\mb{R}^m$. We will reserve the tilde notation for the rows of a matrix. Then, we can write the matrix as the following,
    \begin{equation}
        \mf{A} = \bmxc \tilde{\mf{a}}_1 & \tilde{\mf{a}}_2 & \ldots & \tilde{\mf{a}}_n \emx^\top = \bmxc \tilde{\mf{a}}_1^\top \\ \tilde{\mf{a}}_2^\top \\ \vdots \\ \tilde{\mf{a}}_n^\top \emx
        \label{eq:ch02-mat-row-rep}
    \end{equation}
\end{itemize}

\begin{boxedstuff}
    \begin{problem}
        \textbf{Elements of a row and column of a matrix $\mf{A}$} Consider a matrix $\mf{A} \in \mb{R}^{n \times m}$ with elements $a_{ij}$. Can you write down the elements of the $k^{th}$ row and the $l^{th}$ row of the matrix $\mf{A}$?.
    \end{problem}
\end{boxedstuff}
We will first start with a simplified versions of the matric multiplication problem before tackling the most general problem. It is left as an exercise for you to verify that the following four simplified version comply with Eq.~\ref{eq:ch02-mat-mult}.
\begin{itemize}
    \item \textbf{Inner product:} We have already seen this in the previous chapter. Consider two column vectors $\mf{x}, \mf{y} \in \mb{R}^n$. We can this of these two as $n \times 1$ matrices. The inner product is a product between a row matrix and a column matrix. It is defined as follows, 
    \[ \mf{x}^\top\mf{y} = \sum_{i=1}^{n} x_iy_i \]
    Note that the matrices $\mf{x}^\top \in \mb{R}^{1 \times n}$ and $\mf{y} \in \mb{R}^{n \times 1}$. Thus, matrix multiplication is defined.
    
    \item \textbf{Post-multiplying a matrix $\mf{A}$ by a column vector $\mf{x}$:} Consider a matrix $\mf{A} \in \mb{R}^{n \times m}$ and a column vector $\mf{x} \in \mb{R}^m$. The product $\mf{A}\mf{x}$ produces a column vector $\mf{y} \in \mb{R}^n$.
    \begin{equation}
        \mf{y} = \mf{A}\mf{x} = \sum_{i=1}^n x_i\mf{a}_i = x_1 \mf{a}_1 + x_2 \mf{a}_2 + \cdots + x_m\mf{a}_m
        \label{eq:ch02-mat-post-mult-col-vec}
    \end{equation}
    The above equation has a nice interpreation. The vector $\mf{y}$ is the linear combination of the columns of $\mf{A}$, where the mixture for the linear combination come from the elements of the vector $\mf{x}$. This means that post-multiplying a matrix $\mf{A}$ by a column vector is a process of generating a vector from the span of the set formed by the columns of the matrix $\mf{A}$ (recall span of a set of vector from Section~\ref{sec:ch01-span}). We will now show how Eq.~\ref{eq:ch02-mat-post-mult-col-vec} is the consequence of Eq.~\ref{eq:ch02-mat-mult}. From, Eq.~\ref{eq:ch02-mat-mult}, we have:
    \[ \begin{split}
        \mf{y} &= \mf{A}\mf{x} = \bmxc \sum_{i=1}^m a_{1i}x_i \\ \sum_{i=1}^m a_{2i}x_i \\ \vdots \\ \sum_{i=1}^m a_{ni}x_i \emx = \bmxc a_{11}x_1 + a_{12}x_2 + \cdots + a_{1m}x_m \\ a_{21}x_1 + a_{22}x_2 + \cdots + a_{2m}x_m \\ \vdots \\ a_{n1}x_1 + a_{n2}x_2 + \cdots + a_{nm}x_m \emx = \bmxc a_{11}x_1 \\ a_{21}x_1 \\ \vdots \\ a_{n1}x_1\emx + \bmxc a_{12}x_2 \\ a_{22}x_2 \\ \vdots \\ a_{n2}x_2\emx + \cdots + \bmxc a_{1m}x_m \\ a_{2m}x_m \\ \vdots \\ a_{nm}x_m \emx \\
        &= x_1 \bmxc a_{11} \\ a_{21} \\ \vdots \\ a_{n1}\emx + x_2 \bmxc a_{12} \\ a_{22} \\ \vdots \\ a_{n2}\emx + \cdots + x_m\bmxc a_{1m} \\ a_{2m} \\ \vdots \\ a_{nm} \emx = x_1 \mf{a}_1 + x_2 \mf{a}_2 + \cdots + x_m\mf{a}_m
    \end{split} \]

    \item \textbf{Pre-multiplying a matrix $\mf{A}$ by a row vector $\mf{x}^\top$:} Consider a matrix $\mf{A} \in \mb{R}^{n \times m}$ and a row vector $\mf{x}^\top \in \mb{R}^{1 \times n}$. The product $\mf{x}^\top\mf{A}$ produces a row vector $\mf{y}^\top$, with $\mf{y} \in \mb{R}^{1 \times m}$. 
    \begin{equation}
        \mf{y}^\top = \mf{x}^\top\mf{A} = \sum_{i=1}^m x_i\tilde{\mf{a}}_i^\top = x_1 \tilde{\mf{a}}_1^\top + x_2 \tilde{\mf{a}}_2^\top + \cdots + x_n\tilde{\mf{a}}_n^\top
        \label{eq:ch02-mat-pre-mult-row-vec}
    \end{equation}
    The row vector $\mf{y}^\top$ is a linear combination of the rows of the matrix $\mf{A}$, where the miscture fot the linear combination comes from the elements of the row vector $\mf{x}^\top$. Pre-multiplying a matrix by a row vector is a process of generating a vector from the span of the set formed by the rows of the matrix $\mf{A}$. We will leave it as a exercise for you to show how how Eq.~\ref{eq:ch02-mat-pre-mult-row-vec} is the consequence of Eq.~\ref{eq:ch02-mat-mult}.

    \item \textbf{Outer product:} The outer product, like the inner product, is a product between a row matrix and a column matrix, except in the reverse order. Consider two column vectors $\mf{x}, \mf{y} \in \mb{R}^n$. The outer product is defined as follows,
    \begin{equation}
        \mf{x}\mf{y}^\top = \bmxc x_1 \\ x_2 \\ \vdots \\ x_n \emx \bmxc y_1 & y_2 & \ldots & y_n \emx = \bmxc x_1y_1 & x_1y_2 & \ldots & x_1y_n \\ x_2y_1 & x_2y_2 & \ldots & x_2y_n \\ \vdots & \vdots & \ddots & \vdots \\ x_ny_1 & x_ny_2 & \ldots & x_ny_n \emx \in \mb{R}^{n \times n}
        \label{eq:ch02-mat-outer-prod}
    \end{equation}
    While the inner product produces a scalar, the outer product produces a matrix. The outer product is a very useful operation in many applications, as we will see in the multiple occasions in this course.
\end{itemize}

\begin{boxedstuff}
    \begin{problem}
        \textbf{Outer product to produce a rectangular matrix}. In the Eq.~\ref{eq:ch02-mat-outer-prod}, the outer product operation produces a square matrix. Can the outer product operation ever produce a rectangular matrix of shape $n \times m$. If yes, how? If no, explain why not.
    \end{problem}
\end{boxedstuff}
We now look at some numerical examples to get a handle on the ideas discussed above.

\begin{boxedstuff}
    \begin{example}
        \textbf{Inner product} Let $\mf{x} = \bmxc 1 \\ 3 \\ 4 \emx$ and $\mf{y} = \bmx -1 \\ 2 \\ 1 \emx$. The inner product of these two vectors or column matrices is as follows:
        \[ \mf{x}^\top\mf{y} = \bmxc 1 & 3 & 4 \emx \bmxc -1 \\ 2 \\ 1 \emx = 1 \cdot \pp{-1} + 3 \cdot 2 + 4 \cdot 1 = -1 + 6 + 4 = 9\]
        \label{example:ch02-mat-mul-inn-prod}
    \end{example}

    \begin{example}
        \textbf{Post-mulitplying a matrix by a column vector} Let $\mf{x} = \bmxc 1 \\ 3 \emx$ and $\mf{A} = \bmx 2 & -1 \\ 1 & 1 \emx$. The product $\mf{A}\mf{x}$ is given as follows:
        \[ \mf{y} = \mf{A}\mf{x} = \bmx 2 & -1 \\ 1 & 1 \emx \bmxc 1 \\ 3 \emx = \bmxc 2 \cdot 1 + -1 \cdot 3 \\ 1 \cdot 1 + 1 \cdot 3 \emx = \bmx -1 \\ 4 \emx = 1\bmx 2 \\ 1\emx + 3\bmx -1 \\ 1\emx \]
        \label{example:ch02-mat-post-mul-col-vec}
    \end{example}

    \begin{example}
        \textbf{Pre-mulitplying a matrix by a row vector} Let $\mf{x}^\top = \bmxc 1 & 3 \emx$ and $\mf{A} = \bmx 2 & -1 \\ 1 & 1 \emx$. The product $\mf{A}\mf{x}$ is given as follows:
        \[ \mf{y}^\top = \mf{x}^\top\mf{A} = \bmxc 1 & 3 \emx \bmx 2 & -1 \\ 1 & 1 \emx = \bmxc 1 \cdot 2 + 3 \cdot 1 & 1 \cdot \pp{-1} + 3 \cdot 1 \emx = \bmx 5 & 2 \emx = 1\bmx 2 & -1\emx + 3\bmx 1 & 1 \emx \]
        \label{example:ch02-mat-pre-mul-row-vec}
    \end{example}

    \begin{example}
        \textbf{Outer product} Let $\mf{x} = \bmxc 1 \\ 3 \\ 4 \emx$ and $\mf{y} = \bmx -1 \\ 2 \\ 1 \emx$. The outer product of these two vectors or column matrices is as follows:
        \[ \mf{x}\mf{y}^\top = \bmxc 1 \\ 3 \\ 4 \emx \bmxc -1 & 2 & 1 \emx = \bmxc 1 \cdot \pp{-1} & 1 \cdot 2 & 1 \cdot 1 \\ 3 \cdot \pp{-1} & 3 \cdot 2 & 3 \cdot 1 \\ 4 \cdot \pp{-1} & 4 \cdot 2 & 4 \cdot 1 \emx = \bmxc -1 & 2 & 1 \\ -3 & 6 & 3 \\ -4 & 8 & 4 \emx \]
        \label{example:ch02-mat-outer-prod}
    \end{example}
\end{boxedstuff}

Now we are ready to handle the general matrix multiplication problem. Consider two matrices $\mf{A} \in \mb{R}^{n \times p}$ and $\mf{B} \in \mb{R}^{p \times m}$. These two matrices can be multiplied to get a matrix $\mf{C} = \mf{A}\mf{B} \in \mb{R}^{n \times m}$. We can write the matrices $\mf{A}$ and $\mf{B}$ as follows:
\begin{equation}
    \mf{A} = \bmxc \mf{a}_1 & \mf{a}_2 & \ldots & \mf{a}_p \emx = \bmxc \tilde{\mf{a}}_1^\top \\ \tilde{\mf{a}}_2^\top \\ \vdots \\ \tilde{\mf{a}}_n^\top \emx , \quad \quad\mf{B} = \bmxc \mf{b}_1 & \mf{b}_2 & \ldots & \mf{b}_m \emx = \bmxc \tilde{\mf{b}}_1^\top \\ \tilde{\mf{b}}_2^\top \\ \vdots \\ \tilde{\mf{b}}_p^\top \emx
    \label{eq:ch02-mat-AB-view}
\end{equation}
There four ways to interpret the matrix multiplication operation: inner-product interpretation, column interpretation, row interpretation, and the outer-product interpretation.

\subsubsection{Inner product interpretation}
The $ij^{th}$ element of $\mf{C} = \mf{A}\mf{B}$ is given by the inner product of the $i^{th}$ row of $\mf{A}$ and the $j^{th}$ column of $\mf{B}$.
\begin{equation}
    c_{ij} = \tilde{\mf{a}}_i^\top \mf{b}_j = \sum_{k=1}^{p} a_{ik}b_{kj} \implies \mf{C} = \bmxc \tilde{\mf{a}}_1^\top\mf{b}_1 & \tilde{\mf{a}}_1^\top\mf{b}_2 & \ldots & \tilde{\mf{a}}_1^\top\mf{b}_m \\
    \tilde{\mf{a}}_2^\top\mf{b}_1 & \tilde{\mf{a}}_2^\top\mf{b}_2 & \ldots & \tilde{\mf{a}}_2^\top\mf{b}_m \\
    \vdots & \vdots & \ddots & \vdots \\
    \tilde{\mf{a}}_n^\top\mf{b}_1 & \tilde{\mf{a}}_n^\top\mf{b}_2 & \ldots & \tilde{\mf{a}}_n^\top\mf{b}_m
    \emx
    \label{eq:ch02-mat-mult-inner-prod}
\end{equation}
In fact, there is beautiful way of looking at this. If we express $\mf{A}$ as a block column and $\mf{B}$ as a block row, then we have an outer-product between the block column and the block row.
\begin{equation}
    \mf{C} = \mf{A}\mf{B} = \bmxc \tilde{\mf{a}}_1^\top \\ \tilde{\mf{a}}_2^\top \\ \vdots \\ \tilde{\mf{a}}_n^\top \emx \bmx \mf{b}_1 & \mf{b}_2 & \ldots & \mf{b}_m \emx = \bmxc \tilde{\mf{a}}_1^\top\mf{b}_1 & \tilde{\mf{a}}_1^\top\mf{b}_2 & \ldots & \tilde{\mf{a}}_1^\top\mf{b}_m \\
    \tilde{\mf{a}}_2^\top\mf{b}_1 & \tilde{\mf{a}}_2^\top\mf{b}_2 & \ldots & \tilde{\mf{a}}_2^\top\mf{b}_m \\
    \vdots & \vdots & \ddots & \vdots \\
    \tilde{\mf{a}}_n^\top\mf{b}_1 & \tilde{\mf{a}}_n^\top\mf{b}_2 & \ldots & \tilde{\mf{a}}_n^\top\mf{b}_m
    \emx
    \label{eq:ch02-mat-mult-inner-prod-vec}
\end{equation}
Note that each entry of this block outer-product matrix is an inner product of of two vectors from $\mb{R}^p$.

\subsubsection{Column interpretation}
The columns of the matrix $\mf{C} = \mf{A}\mf{B}$ are the linear combinations of the columns of the matrix $\mf{A}$, where the mixture for the linear combination come from the columns of the matrix $\mf{B}$.
\begin{equation}
    \mf{c}_i = \mf{A}\mf{b}_i \implies \bmx \mf{c}_1 & \mf{c}_2 & \cdots & \mf{c}_m \emx = \mf{A} \bmx \mf{b}_1 & \mf{b}_2 & \cdots & \mf{b}_m \emx = \bmx \mf{A}\mf{b}_1 & \mf{A}\mf{b}_2 & \cdots & \mf{A}\mf{b}_m \emx
    \label{eq:ch02-mat-mult-col-interp}
\end{equation}

\subsubsection{Row interpretation}
The rows of the matrix $\mf{C} = \mf{A}\mf{B}$ are the linear combinations of the rows of the matrix $\mf{B}$, where the mixture for the linear combination come from the rows of the matrix $\mf{A}$.
\begin{equation}
    \tilde{\mf{c}}_i^\top = \mf{a}_i^\top\mf{B} \implies \bmxc \tilde{\mf{c}}_1^\top \\ \tilde{\mf{c}}_2^\top \\ \vdots \\ \tilde{\mf{c}}_n^\top \emx = \bmxc \mf{a}_1^\top \\ \mf{a}_2^\top \\ \vdots \\ \mf{a}_n^\top \emx \mf{B} = \bmxc \mf{a}_1^\top\mf{B} \\ \mf{a}_2^\top\mf{B} \\ \vdots \\ \mf{a}_n^\top\mf{B} \emx
    \label{eq:ch02-mat-mult-row-interp}
\end{equation}

\subsubsection{Outer product interpretation}
The outer-product view is complementary to the inner-product view. If we express the matrices $\mf{A}$ and $\mf{B}$ as block column and block row matrix, then the matrix $\mf{C} = \mf{A}\mf{B}$ is the outer-product of the block column and the block row.
\begin{equation}
    \mf{C} = \mf{A}\mf{B} = \bmxc \mf{a}_1 & \mf{a}_2 & \ldots & \mf{a}_p \emx \bmxc \tilde{\mf{b}}_1^\top \\ \tilde{\mf{b}}_2^\top \\ \ldots \\ \tilde{\mf{b}}_p^\top \emx = \mf{a}_1\tilde{\mf{b}}_1^\top + \mf{a}_2\tilde{\mf{b}}_2^\top + \cdots + \mf{a}_p\tilde{\mf{b}}_p^\top
    \label{eq:ch02-mat-mult-outer-prod}
\end{equation}
In this view, we see that matrix mulitplication can be viewed the inner-product of a block row matrix with a block column matrix, while the individual elements of the inner-product sum are the outer-products.

Each of these four views will be useful under different circumstances to help interpret the meaning of the matrix mulitplication operation. We will now look at some numerical examples for the four views to get comfortable with them.

\begin{boxedstuff}
    \begin{example}
        \textbf{Four interpretations of matrix multiplication} Let $\mf{A} = \bmx 1 & 2 \\ 3 & 4 \emx$ and $\mf{B} = \bmx 5 & 6 \\ 7 & 8 \emx$. The matrix multiplication $\mf{C} = \mf{A}\mf{B}$ is given as follows:
        \begin{itemize}
            \item \textbf{Inner product interpretation:}
            \[ \begin{split}
                \mf{C} &= \mf{A}\mf{B} = \bmxc 1 & 2 \\ 3 & 4 \emx \bmxc 5 & 6 \\ 7 & 8 \emx = \bmxc 
                \bmxc 1 & 2 \emx \bmx 5 \\ 7 \emx & \bmxc 1 & 2 \emx \bmx 6 \\ 8 \emx \\ 
                \bmxc 3 & 4 \emx \bmx 5 \\ 7 \emx & \bmxc 3 & 4 \emx \bmx 6 \\ 8 \emx \emx = \bmxc 1 \cdot 5 + 2 \cdot 7 & 1 \cdot 6 + 2 \cdot 8 \\ 3 \cdot 5 + 4 \cdot 7 & 3 \cdot 6 + 4 \cdot 8 \emx = \bmxc 19 & 22 \\ 43 & 50 \emx
            \end{split} \]
    
            \item \textbf{Column interpretation:}
            \[ \begin{split}
                \mf{C} &= \mf{A}\mf{B} = \bmxc 1 & 2 \\ 3 & 4 \emx \bmx 5 & 6 \\ 7 & 8 \emx = \bmx 
                5\bmx 1 \\ 3\emx + 7\bmx 2 \\ 4\emx & 6\bmx 1 \\ 3\emx + 8\bmx 2 \\ 4\emx \emx = \bmxc 19 & 22 \\ 43 & 50 \emx
            \end{split} \]
            
            \item \textbf{Row interpretation} 
            \[ \begin{split}
                \mf{C} &= \mf{A}\mf{B} = \bmxc 1 & 2 \\ 3 & 4 \emx \bmx 5 & 6 \\ 7 & 8 \emx = \bmx 
                1\bmx 5 & 6\emx + 2\bmx 7 & 8\emx \\ 
                3\bmx 5 & 6\emx + 4\bmx 7 & 8\emx \emx = \bmxc 19 & 22 \\ 43 & 50 \emx
            \end{split} \]
            
            \item \textbf{Outer product interpretation} 
            \[ \begin{split}
                \mf{C} &= \mf{A}\mf{B} = \bmxc 1 & 2 \\ 3 & 4 \emx \bmx 5 & 6 \\ 7 & 8 \emx = \bmx1 \\ 3\emx\bmx 5 & 6\emx + \bmx2 \\ 4\emx\bmx 7 & 8\emx = \bmxc 5 & 6 \\ 15 & 18 \emx + \bmxc 14 & 16 \\ 28 & 32 \emx = \bmxc 19 & 22 \\ 43 & 50 \emx
            \end{split} \]
        \end{itemize}
        \label{example:ch02-mat-mult}
    \end{example}
\end{boxedstuff}

\subsubsection{Properties of matrix multiplication}
We will not demonstrate any of these properties, but we will list them here for your reference. It is left as an exercise for you to demonstrate these properties. Here are some of the properties of matrix multiplication:
\begin{itemize}
    \item \textbf{Non-commutative property:} $\mf{A}\mf{B} \neq \mf{B}\mf{A}$. You could simply use the matrices in Example~\ref{example:ch02-mat-mult} to verify that this is true. \textcolor{red}{Can give an example of two matrices where their product does commute?}.
    \item \textbf{Dsitributive}: $\mf{A}\pp{\mf{B} + \mf{C}} = \mf{A}\mf{B} + \mf{A}\mf{C}$ and $\pp{\mf{A} + \mf{B}}\mf{C} = \mf{A}\mf{C} + \mf{B}\mf{C}$.
    \item \textbf{Associative}: $\mf{A}\pp{\mf{B}\mf{C}} = \pp{\mf{A}\mf{B}}\mf{C}$
    \item \textbf{Transpose of the product:} $\pp{\mf{A}\mf{B}}^\top = \mf{B}^\top\mf{A}^\top$.
    \item \textbf{Identity matrix:} $\mf{I}_n\mf{A} = \mf{A}\mf{I}_m = \mf{A}$
    \item \textbf{Scalar multiplication:} $\pp{\alpha\mf{A}}\mf{B} = \alpha\pp{\mf{A}\mf{B}} = \mf{A}\pp{\alpha\mf{B}}$
\end{itemize}

\section{Rank of a matrix}
The rank of a matrix is a very important concept in linear algebra. There are different ways of defining it, but we will use a geometric definition that is easy to wrap our head around.  

The \textit{rank of a matrix} $\mf{A} \in \mb{R}^{n \times m}$ is the dimension of the subspace spanned by the set of columns of $\mf{A}$ or the set of rows of $\mf{A}$.
\begin{equation}
    \begin{split}
    \text{rank}\pp{\mf{A}} &= \text{dim}\pp{\text{span}\pp{\lc \mf{a}_1, \mf{a}_2, \ldots, \mf {a}_m} \rc} \longrightarrow \text{Column rank}\\ 
    &= \text{dim}\pp{\text{span}\pp{\lc \tilde{\mf{a}}_1, \tilde{\mf{a}}_2, \ldots, \tilde{\mf{a}}_n} \rc} \longrightarrow \text{Row rank} \\
    \leq \min\pp{n, m}
    \end{split}
    \label{eq:ch02-mat-rank}
\end{equation}

This is also equivalent to saying it is the smallest number of columns or rows of $\mf{A}$ that form  a linearly independent set; basically, the number of linearly independent columns or rows of the matrix $\mf{A}$. A matrix $\mf{A} \in \mb{R}^{n \times m}$ is said to be a \textbf{full rank matrix} if $\text{rank}\pp{\mf{A}} = \min\pp{n, m}$. A matrix is said to be \textbf{rank deficient} if $\text{rank}\pp{\mf{A}} < \min\pp{n, m}$.

\section{Matrix inverse}
We will talk about matrix inverses in more detail in Chapter \hl{xxx}, but we will conclude this chapter with a provide brief discussion of this concept. Consider a square matrix $\mf{A} \in \mb{R}^{n \times n}$ that is full rank (if so, what is the rank of $\mf{A}$?). A matrix $\mf{B} \in \mb{R}^{n \times n}$ is called the \textit{inverse} of the matrix $\mf{A}$ if and only if $\mf{A}\mf{B} = \mf{B}\mf{A} = \mf{I}_n$. This matrix $\mf{B}$ is denoted by $\mf{A}^{-1}$. 

All full rank square matrices have a unique inverse. These matrices are said to be \textit{invertible} or \textit{non-singular}, which rank deficient square matrices are said to \textit{non-invertible} or \textit{singular}. For a non-singular matrix $\mf{A}$, $\mf{A}$ and $\mf{A}^{-1}$ are inverses of each other. Because both pre- and post-multiplying $\mf{A}^{-1}$ with $\mf{A}$ gives us the identity matrix, we say $\mf{A}^{-1}$ is both the left and right inverse of $\mf{A}$. We will see in Chapter \hl{xxx} that full rectangular matrices can either have non-unique left or right inverses, and not both.

The inverse of matrix follows the following two properites, which you are en encouraged to demonstrate:
\begin{itemize}
    \item $\pp{\mf{A}^{-1}}^{-1} = \mf{A}$
    \item $\pp{\mf{A}\mf{B}}^{-1} = \mf{B}^{-1}\mf{A}^{-1}$
    \item $\pp{\mf{A}^{-1}}^{\top} = \pp{\mf{A}^{\top}}^{-1}$
\end{itemize}

\section{Exercise}
\vspace{-0.5cm}
\begin{center}
    \rule{\textwidth}{1pt}
\end{center}

\begin{enumerate}
    \item Elements of the matrix $\mf{C} \in \mb{R}^{m \times n}$ obtained as the product of two matrices $\mf{A} \in \mb{R}^{m \times p}$ and $\mf{B} \in \mb{R}^{p \times n}$ is given by,
    \[ c_{ij} = \sum_{k=1}^{p}a_{ik}b_{kj} \]
    We had discussed four different ways to think of matrix multiplication. By algebraically manipulating the previous equation, arrive at these four views (inner product view, column view, row view and outer product view)?
    
    \item Show that $\ct{\mf{A}\mf{B}}^\top = \mf{B}^\top\mf{A}^\top$.
    
    \item Derive the inverse of the matrix $\mf{A} = \begin{bmatrix}a & b\\c & d\end{bmatrix}$.

    \item Prove that the rank of an outer product $\mf{x}\mf{y}^\top$ is 1, where $\mf{x},\mf{y} \in \mb{R}^n$ and $\mf{x}, \mf{y} \neq \mf{0}$.  \textbf{[Marks: 1]}
    
    \item For a $n \times n$ square matrix $\mf{A}$, prove that if $\mf{A}\mf{X} = \mf{I}$, then $\mf{X}\mf{A} = \mf{I}$ and $\mf{X} = \mf{A}^{-1}$.

    \item Prove the following for the non-singular square matrices $\mf{A}$ and $\mf{B}$:
    \begin{enumerate}
        \item $\mf{A}\mf{B}$ is non-singular.
        \item $\ct{\mf{A}^{-1}}^{-1} = \mf{A}$.
        \item $\ct{\mf{A}\mf{B}}^{-1} = \mf{B}^{-1}\mf{A}^{-1}$
        \item $\ct{\mf{A}^\top}^{-1} = \ct{\mf{A}^{-1}}^\top$
    \end{enumerate}
    
    \item \textbf{Computational cost of different operations.} What is the computational cost of the following matrix operations? Computational cost refers to the number of arithmetic operations  required to carry out a particular matrix operation. Computational cost is a measure of the efficiency of an algorithm. For example, consider the operation of vector addition, $\mf{a} + \mf{b}$, where $\mf{a}, \mf{b} \in \mb{R}^n$. This requires $n$ addition/subtraction operations and zero multiplication/division operations.
    \begin{enumerate}
        \item Matrix multiplication: $\mf{AB}$, where $\mf{A}, \mf{B} \in \mb{R}^{n \times n}$
        \item Inner product: $\mf{u}^\top\mf{v}$
    \end{enumerate} 
    Report the counts for the addition/subtraction and multiplication/division operations separately.
    
    \item Consider the following matrix,
    \[ \mf{A} = \bmx \frac{\sqrt{3}}{2} & -\frac{1}{2}\\\frac{1}{2} & \frac{\sqrt{3}}{2} \emx \bmx 0.1 & 0\\0 & 0.9 \emx \bmx \frac{\sqrt{3}}{2} & \frac{1}{2}\\-\frac{1}{2} & \frac{\sqrt{3}}{2} \emx \]
    Find out the expression for $\mf{A}_n = \mf{A}^n$. What is $\mf{A}_\infty = \lim_{n\to\infty} \mf{A}^n$?

    \item Prove that a matrix $\mf{M} \in \mathbb{R}^{n \times n}$ can always be written as a sum of a symmetric matrix $\mf{S}$ and a skew-symmetric matrix $\mf{A}$.
    \[ \mf{M} = \mf{S} + \mf{A}, \,\,\, \mf{S}^\top = \mf{S} \, \text{ and } \, \mf{A}^\top = -\mf{A} \]

    \item The trace of a matrix $\mf{A} \in \mb{R}^{n \times n}$ is defined as, $trace\left(\mf{A}\right) = \sum_{i=1}^{n}a_{ii}$. Prove the following,
    \begin{enumerate}
        \item $trace\left(\mf{A}\right)$ is a linear function of $\mf{A}$.
        \item $trace\left(\mf{AB}\right) = trace\left(\mf{BA}\right)$
        \item $trace\left(\mf{A}^\top\mf{A}\right) = 0 \implies \mf{A} = 0$
    \end{enumerate}

    \item Consider upper triangular and lower triangular matrices $\mf{U}$ and $\mf{L}$, respectively.
    \begin{enumerate}
        \item Is the product of two upper triangular matrices $\mf{U}_1\mf{U}_2$ upper triangular?
        \item Is the product of two lower triangular matrices $\mf{L}_1\mf{L}_2$ upper triangular?
        \item What is the $trace\lp \mf{L}\mf{U} \rp$?
    \end{enumerate}

    \item Consider the following upper-triangular matrix, 
    \[ \mf{U} = \begin{bmatrix}
    u_{11} & u_{12} & u_{13} & \cdots & u_{1n}\\
    0 & u_{22} & u_{23} & \cdots & u_{2n}\\
    0 & 0 & u_{33} & \cdots & u_{3n}\\
    \vdots & \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & 0 & \cdots & u_{nn}\\\end{bmatrix} \]
    where, $u_{ii} \neq 0, \,\,\, 1 \leq i \leq n$. Do the columns of this matrix form a linearly independent set? Explain your answer.

    \item Verify that $\mf{A}$ and $\mf{B}$ are inverses of each other,
    \begin{enumerate}
        \item $\mf{A} = \mf{I} - \mf{u}\mf{v}^\top$ and $\mf{B} = \mf{I} + \mf{u}\mf{v}^\top / \lp 1 - \mf{v}^\top\mf{u} \rp$
        \item $\mf{A} = \mf{C} - \mf{u}\mf{v}^\top$ and $\mf{B} = \mf{C}^{-1} + \mf{C}^{-1}\mf{u}\mf{v}^\top\mf{C}^{-1} / \lp 1 - \mf{v}^\top\mf{C}^{-1}\mf{u} \rp$
        \item $\mf{A} = \mf{I} - \mf{U}\mf{V}$ and $\mf{B} = \mf{I}_{n} + \mf{U}\lp \mf{I}_m - \mf{V}\mf{U}\rp^{-1}\mf{V}$
        \item $\mf{A} = \mf{C} - \mf{U}\mf{D}^{-1}\mf{V}$ and $\mf{B} = \mf{C}^{-1} + \mf{C}^{-1}\mf{U}\lp \mf{D} - \mf{V}\mf{C}^{-1}\mf{U}\rp^{-1}\mf{V}\mf{C}^{-1}$
    \end{enumerate}
    where, $\mf{A}, \mf{B}, \mf{C} \in \mb{R}^{n \times n}$, $\mf{u}, \mf{v} \in \mb{R}^n$, $\mf{U} \in \mb{R}^{n \times m}$, $\mf{V} \in \mb{R}^{m \times n}$ and $\mf{D} \in \mb{R}^{m \times m}$.
\end{enumerate}